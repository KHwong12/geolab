{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec28f8d",
   "metadata": {},
   "source": [
    "# Lab 01 - Introduction to pandas\n",
    "\n",
    "**Author**: Kenneth Wong (kennethwong12.netlify.app)\n",
    "\n",
    "**Last Edited**: 2021-10-25\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5256186",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d4fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53717a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook serves as a guide on how to **read and explore tabular data files in Python**. We will focus on using [pandas](https://pandas.pydata.org/pandas-docs/stable/) which is an open-source package for data analysis in Python. pandas is an excellent toolkit for working with **real world data** that often have a tabular structure (rows and columns).\n",
    "\n",
    "We will first get familiar with the **pandas data structures**: *DataFrame* and *Series*:\n",
    "\n",
    "![pandas data structures](https://geo-python-site.readthedocs.io/en/latest/_images/pandas-structures.png)\n",
    "\n",
    "- **pandas DataFrame** (a 2-dimensional data structure) is used for storing and mainpulating table-like data (data with rows and columns) in Python. You can think of a pandas DataFrame as a programmable spreadsheet. \n",
    "- **pandas Series** (a 1-dimensional data structure) is used for storing and manipulating a sequence of values. pandas Series is kind of like a list, but more clever. One row or one column in a pandas DataFrame is actually a pandas Series. \n",
    "\n",
    "These pandas structures incorporate a number of things we've already encountered, such as indices, data stored in a collection, and data types. Let's have another look at the pandas data structures below with some additional annotation.\n",
    "\n",
    "![pandas data structures annotated](https://geo-python-site.readthedocs.io/en/latest/_images/pandas-structures-annotated.png)\n",
    "\n",
    "As you can see, both DataFrames and Series in pandas have an index that can be used to select values, but they also have column labels to identify columns in DataFrames. In the lesson this week we'll use many of these features to explore real-world data and learn some useful data analysis procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add971d",
   "metadata": {},
   "source": [
    "## The dataset: weather statistics\n",
    "\n",
    "Our input data is a text file containing weather observations from Kumpula, Helsinki, Finland retrieved from [NOAA](https://www.ncdc.noaa.gov/):\n",
    "\n",
    "- File name: [Kumpula-June-2016-w-metadata.txt](Kumpula-June-2016-w-metadata.txt) (have a look at the file before reading it in using pandas!)\n",
    "- The file is available in the binder and CSC notebook instances, under the L5 folder \n",
    "- The data file contains observed daily mean, minimum, and maximum temperatures from June 2016 recorded from the Kumpula weather observation station in Helsinki.\n",
    "- There are 30 rows of data in this sample data set.\n",
    "- The data has been derived from a data file of daily temperature measurments downloaded from [NOAA](https://www.ncdc.noaa.gov/cdo-web/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f6f2c",
   "metadata": {},
   "source": [
    "## Reading a data file with pandas\n",
    "\n",
    "Now we're ready to read in our temperature data file. **First, we need to import the pandas module.** It is customary to import pandas as `pd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cbf8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd578e55",
   "metadata": {},
   "source": [
    "**Next, we'll read the input data file**, and store the contents of that file into a variable called `data` Using the `pandas.read_csv()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76723bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file using pandas\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/geo-python/site/master/source/notebooks/L5/Kumpula-June-2016-w-metadata.txt', skiprows = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18efe91a",
   "metadata": {},
   "source": [
    "`pandas.read_csv()` is a general function for reading data files separated by commas, spaces, or other common separators. \n",
    "\n",
    "pandas has several different functions for parsing input data from different formats. There is, for example, a separate function for reading Excel files `read_excel`. Another useful function is `read_pickle` for reading data stored in the [Python pickle format](https://docs.python.org/3/library/pickle.html). Check out the [pandas documentation about input and output functions](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-tools-text-csv-hdf5) and Chapter 6 in [MacKinney (2017): Python for Data Analysis](https://geo-python-site.readthedocs.io/en/latest/course-info/resources.html#books) for more details about reading data.\n",
    "\n",
    "If all goes as planned, you should now have a new variable `data` in memory that contains the input data. \n",
    "\n",
    "Let's check the the contents of this variable by calling `data` or `print(data)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a1ca7c",
   "metadata": {},
   "source": [
    "After reading in the data, it is always good to check that everything went well by printing out the data as we did here. However, often it is enough to have a look at the top few rows of the data. \n",
    "\n",
    "We can use the `.head()` function of the pandas DataFrame object to quickly check the top rows. By default, the `.head()` function returns the first 5 rows of the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732edfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d148909",
   "metadata": {},
   "source": [
    "Note that pandas DataFrames have **labelled axes (rows and columns)**. In our sample data, the rows labeled with an index value (`0` to `29`), and columns labelled `YEARMODA`, `TEMP`, `MAX`, and `MIN`. Later on, we will learn how to use these labels for selecting and updating subsets of the data.\n",
    "\n",
    "Let's also confirm the data type of our data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204394a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642e76f",
   "metadata": {},
   "source": [
    "No surprises here, our data variable is a pandas DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "## DataFrame properties\n",
    "\n",
    "Let's continue with the full data set that we have stored in the variable `data` and explore it's contents further. \n",
    "A normal first step when you load new data is to explore the dataset a bit to understand how the data is structured, and what kind of values are stored in there.\n",
    "\n",
    "### Length (number of rows/columns)\n",
    "\n",
    "Let's start by checking the size of our data frame. We can use the `len()` function similar to the one we use with lists to check how many rows we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06ba396",
   "metadata": {},
   "source": [
    "We can also get a quick sense of the size of the dataset using the `shape` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa525b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataframe shape (number of rows, number of columns)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91957860",
   "metadata": {},
   "source": [
    "Here we see that our dataset has 30 rows and 4 columns, just as we saw above when printing out the entire DataFrame.\n",
    "\n",
    "`shape` is one of the several [attributes related to a pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#attributes-and-underlying-data).\n",
    "\n",
    "### Column Names\n",
    "\n",
    "**We can also check the column names we have in our DataFrame.** We already saw the column names when we checked the 5 first rows using `data.head()`, but often it is useful to access the column names directly. You can check the column names by calling `data.columns` (returns an index object that contains the column labels) or `data.columns.values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe317e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print column names\n",
    "data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56ae87",
   "metadata": {},
   "source": [
    "We can also find information about the row identifiers using the `index` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print index\n",
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4cca31",
   "metadata": {},
   "source": [
    "Here we see how the data is indexed, starting at 0, ending at 30, and with an increment of 1 between each value. This is basically the same way in which Python lists are indexed, however, pandas also allows other ways of identifying the rows. DataFrame indices could, for example, be character strings, or date objects. We will learn more about resetting the index later.\n",
    "\n",
    "What about the data types of each column in our DataFrame? We can check the data type of all columns at once using `pandas.DataFrame.dtypes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4153dc",
   "metadata": {},
   "source": [
    "Here we see that `YEARMODA` is an integer value (with 64-bit precision; `int64`), while the other values are all decimal values with 64-bit precision (`float64`).\n",
    "\n",
    "## Selecting columns\n",
    "\n",
    "We can select specific columns based on the column values. The basic syntax is `dataframe[value]`, where value can be a single column name, or a list of column names. Let's start by selecting two columns, `'YEARMODA'` and `'TEMP'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = data[['YEARMODA','TEMP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e5f7c",
   "metadata": {},
   "source": [
    "The subset is still a pandas DataFrame, and we are able to use all the methods and attributes related to a pandas DataFrame also with this subset. For example, we can check the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4707b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00763f",
   "metadata": {},
   "source": [
    "## Descriptive statistics\n",
    "\n",
    "pandas DataFrames and Series contain useful methods for getting summary statistics. Available methods include `mean()`, `median()`, `min()`, `max()`, and `std()` (the standard deviation).\n",
    "\n",
    "We could, for example, check the mean temperature in our input data. We check the mean for a single column (*Series*): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mean value of a column\n",
    "data['TEMP'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286043ea",
   "metadata": {},
   "source": [
    "and for all columns (in the *DataFrame*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mean value for all columns\n",
    "data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bac9be",
   "metadata": {},
   "source": [
    "For an overview of the basic statistics for all attributes in the data, we can use the `describe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17815dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive statistics\n",
    "data.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
